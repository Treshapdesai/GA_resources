{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\"> \n",
    "# Lesson: PyMC & Bayesian Regression\n",
    "\n",
    "_Authors: Tim Book, Matt Brems, Noelle Brown_\n",
    "\n",
    "## LEARNING OBJECTIVES\n",
    "By the end of the lesson, students should be able to:\n",
    "- Define conjugacy.\n",
    "- Fit models in PyMC.\n",
    "- Interpret traceplots, posterior distributions, and posterior predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## Recap\n",
    "Remember that parameters have distributions. In Bayesian statistics, our goal is to **find the posterior distribution of our parameter of interest**.\n",
    "\n",
    "<details><summary>In order to generate a posterior distribution, what are the two components we need?</summary>\n",
    "\n",
    "- Prior distribution summarizing our beliefs about the parameter before observing any data.\n",
    "- Likelihood function summarizing how our data were generated.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Bayes?\n",
    "There are several reasons why we would prefer to use a Bayesian technique over a frequentist one. What are they? (THREAD)\n",
    "\n",
    "<details>\n",
    "    <summary>What are some reasons to use Bayesian techniques?</summary>\n",
    "    \n",
    "* When you have prior knowledge of a situation (ie, you might no better than data suggests)\n",
    "* When you have small data that can benefit from additional subject matter expertise\n",
    "* When you want to be able to make advanced probabilistic statements that are unavailable to frequentist techniques\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pymc3\n",
    "# !pip install pymc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Skittles!\n",
    "In a snack-size pack of Skittles, you are concerned with getting an unfair amount of green Skittles (the worst flavor). You think there are about 20 skittles per pack. There are 5 different colors of Skittles. \n",
    "\n",
    "<img src=\"../images/skittles.jpeg\" style=\"height: 300px\"> \n",
    "\n",
    "How can we determine if we get an unfair amount of green Skittles?\n",
    "\n",
    "**A frequentist would...**\n",
    "- Open many snack-size packs of Skittles, and estimate $\\hat{p}$, the probability of getting a green skittle.\n",
    "- Conduct a hypothesis test to determine if $p = 0.2$.\n",
    "\n",
    "**A Bayesian would...**\n",
    "- Begin with a preconceived notion about $p$ (a prior). This prior reflects not just our belief, but the strength of our belief.\n",
    "- Open many snack-size packs of Skittles and record the results (the likelihood).\n",
    "- Obtain a posterior distribution for $p$ that combines our priors with our data.\n",
    "- Make statements about this posterior.\n",
    "\n",
    "What is the (posterior) distribution of the number of green skittles I might find in a pack?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_greens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_greens\n",
       "0         7\n",
       "1        10\n",
       "2         7\n",
       "3         2\n",
       "4         8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suspend your disbelief and pretend we only have data on the number of\n",
    "# green Skittles in packs, but NOT the total number of Skittles in said packs.\n",
    "skits = pd.read_csv(\"../datasets/skittles.csv\")\n",
    "skits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define our likelihood and priors\n",
    "\n",
    "We'll start with our priors. \n",
    "\n",
    "What do we know/believe about the percentage of green skittles we might find in a pack? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What distribution will be best for this?</summary>\n",
    "\n",
    "- The percentage of green skittles we might find in a pack is a number between 0 and 1.\n",
    "- The Beta distribution will be good here!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Beta Distribution\n",
    "Recall that the beta distribution is a **continuous** distribution. It's often used in Bayesian statistics because it's one of the only continuous distributions that take values between 0 and 1. This makes it particularly convenient to use as a prior on probabilities.\n",
    "\n",
    "You can take a closer look at the Beta distribution [here](https://en.wikipedia.org/wiki/Beta_distribution)!\n",
    "\n",
    "If $X \\sim \\text{Beta}(a, b)$, then\n",
    "\n",
    "* $\\text{E}[X] = \\frac{a}{a + b}$\n",
    "* $\\text{Var}[X] = \\frac{ab}{(a + b)^2(a + b + 1)}$\n",
    "\n",
    "Also note: The $\\text{Beta}(1, 1)$ distribution is actually the $\\text{Uniform}(0, 1)$ distrubtion!\n",
    "\n",
    "Let's compute some beta means and variances. What do you think is a reasonable prior for our problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>In my opinion, setting up your priors and likelihoods is the hardest part of Bayesian inference.</summary>\n",
    "\n",
    "![](../images/modified_bayes.png)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_stats(a, b):\n",
    "    mean_ = a / (a + b)\n",
    "    var_ = a*b / ((a + b)**2 * (a + b + 1))\n",
    "    return mean_, var_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2, 0.007619047619047619)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want the mean to be about 0.2 (20% of each flavor)\n",
    "beta_stats(4, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.20634920634920634, 0.002558893927941547)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_stats(13, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p ~ Beta(4, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>We can also set a prior on the number of Skittles that are in a pack! What distribution will be best for this?</summary>\n",
    "\n",
    "- The number of Skittles in each pack is count data.\n",
    "- The Poisson distribution will work well here!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N ~ Poisson(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define our likelihood. \n",
    "\n",
    "We are interested in seeing how many green Skittles are in a bag.\n",
    "\n",
    "There's a fixed number of Skittles in a bag. Each of those Skittles has some probability of being green or not green.\n",
    "We are looking to see if a Skittle is green or not in our bag. \n",
    "\n",
    "A good tip from [Bayesian Methods for Hackers:](https://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter2_MorePyMC/Ch2_MorePyMC_PyMC3.ipynb)  \n",
    "> \"A good starting thought to Bayesian modeling is to think about how your data might have been generated. Position yourself in an omniscient position, and try to imagine how you would recreate the dataset.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What distribution will be best for this?</summary>\n",
    "\n",
    "- Each Skittle has some probability of being green and some probability of being not green.\n",
    "- The Binomial distribution will work here.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X ~ Binomial(Number of skittles, probability of green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priors:\n",
    "# N ~ Poi(20)\n",
    "# p ~ Beta(10, 50) \n",
    "\n",
    "# Likelihood\n",
    "# X ~ Binomial(Number of skittles, prob of green)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCMC Using PyMC3\n",
    "Markov chain Monte Carlo methods (commonly abbreviated MCMC methods) made conjugacy mostly unnecessary.\n",
    "\n",
    "![](./images/pymc3-res.png)\n",
    "\n",
    "[TL;DR](https://en.wikipedia.org/wiki/TL;DR) of MCMC: We use computers to take our prior and likelihood, then simulate a large number of samples from the posterior.\n",
    "- If I want to find the mean of my posterior distribution, then I can just do `np.mean(xs)`, where `xs` is my saved list of samples from my posterior.\n",
    "- If I want to find the median of my posterior distribution, then I can do `np.median(xs)`.\n",
    "- If I want to find the 95% [credible interval](http://www.statisticshowto.com/credible-interval/) for my parameter based on the posterior distribution of my parameter, then I can just find the 2.5th and 97.5th percentiles of `xs`!\n",
    "\n",
    "Today, we're going to use `PyMC3`, a Bayesian modeling library in Python that will enable us to use Markov chain Monte Carlo methods. Luckily, this is a package that is geared toward scientists, not necessarily statisticians. We'll be able to use what we currently know about Bayesian statistics to leverage this *really* powerful library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Situation:\n",
    "$$\n",
    "\\begin{align}\n",
    "    X | p &\\sim \\text{Binomial}(n, p) \\\\\n",
    "        p &\\sim \\text{Beta}(a, b)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We've also observed a certain number of greens in our 20 bags (our data).\n",
    "\n",
    "### PyMC3 models live in \"contexts\" instead of \"objects\"\n",
    "That means they'll look something like this:\n",
    "\n",
    "```python\n",
    "with pm.Model() as model:\n",
    "    # Define priors\n",
    "    # Define likelihood\n",
    "    # Execute MCMC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Let's set this up in PyMC3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-ac046342c294>:8: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  trace = pm.sample(10000, tune=1000)\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "CompoundStep\n",
      ">Metropolis: [N]\n",
      ">NUTS: [p]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='719' class='' max='22000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      3.27% [719/22000 00:16<08:02 Sampling 2 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pm.Model() as m:\n",
    "    # Priors\n",
    "    N = pm.Poisson(\"N\", 20)\n",
    "    p = pm.Beta(\"p\", 4, 16)\n",
    "    # Likelihood\n",
    "    X = pm.Binomial(\"X\", N, p, observed=skits[\"n_greens\"])\n",
    "    # execute\n",
    "    trace = pm.sample(10000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with m:\n",
    "    pm.traceplot(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected mean that an individual skittle will be grean\n",
    "np.mean(trace[\"p\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected amount of green skittles is number of skittles in bag * prob of green\n",
    "mean_green = trace[\"N\"]*trace[\"p\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mean_green, bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the probability we will get an expected value of between 5 and 6 green skittles?\n",
    "np.mean(\n",
    "    (mean_green <= 6) & (mean_green >= 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Election Day\n",
    "\n",
    "Let's review the election example from the other day using polling data from [FiveThirtyEight](https://projects.fivethirtyeight.com/2020-election-forecast/). This time, we will look at the probability that Trump wins.\n",
    "\n",
    "Reminder: our polls gave us the following totals:\n",
    "\n",
    "```python\n",
    "n_biden = 7_616 # number of successes\n",
    "n_surveys = 14_995 # number of trials\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the results of our polls.\n",
    "# 1 = Trump led the poll\n",
    "# 0 = Biden led the poll\n",
    "polls = np.array([1]*7379 + [0]*7616)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Establish our priors and our likelihood\n",
    "\n",
    "Here we are looking for the percentage of votes Trump is likely to win in the election.\n",
    "\n",
    "An important thing to note is that there is no such thing as sample data for problems like this, since elections only ever happen once. This is why pollsters conduct polls - to gather information about what will _probably_ happen in the future. Polls aren't perfect though, and they also change over time. **We know through personal experience that most elections end up being very close to 50/50.**\n",
    "\n",
    "We can use the Beta distribution for our prior. We need a mean of .5 (because we expect the election to be 50/50).\n",
    "\n",
    "_Hint:_ The priors are as subjective as they can get here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_stats(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior\n",
    "# p ~ Beta(5, 5)\n",
    "\n",
    "# I can even set this up with even more uncertainty:\n",
    "# p ~ Beta(a, b)\n",
    "# a ~ Exp(5)\n",
    "# b ~ Exp(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood\n",
    "# X ~ Bernoulli(p)\n",
    "# OR: X ~ Binomial(number of trials, probability of success)\n",
    "# X ~ Binomial(1, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set up the PyMC3 model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as m:\n",
    "    a = pm.Exponential(\"alpha\", lam=1/5)\n",
    "    b = pm.Exponential(\"beta\", lam=1/5)\n",
    "    p = pm.Beta(\"p\", a, b)\n",
    "    X = pm.Binomial(\"X\", 1, p, observed=polls)\n",
    "    trace = pm.sample(10000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with m:\n",
    "    pm.traceplot(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(trace['p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(trace[\"p\"] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(trace[\"p\"] < 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we didn't take any outside factors into account (which FiveThirtyEight does) or even talk about the electoral college. Read more about FiveThirtyEight's modeling [here](https://fivethirtyeight.com/features/how-fivethirtyeights-2020-presidential-forecast-works-and-whats-different-because-of-covid-19/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Bayesian Regression for Pay Equity\n",
    "It is an often-recited statistic that women are paid about 77 cents per dollar a man makes for equal work. But how is this number (and others like it) calculated? We actually conduct a log-regression based on a person's legitimate skill level (ie, things such as education or years of experience that may legitimately contribute to you being paid more) and an indicator for the protected class. For example:\n",
    "\n",
    "$$ \\log W = \\beta_0 + \\beta_1\\text{skill} + \\beta_2\\text{class} $$\n",
    "\n",
    "After exponentiating both sides:\n",
    "\n",
    "$$ W = e^{\\beta_0 + \\beta_1\\text{skill}}e^{\\beta_2\\text{class}} $$\n",
    "\n",
    "Where $e^{\\beta_2\\text{class}}$ simplifies to:\n",
    "\n",
    "* $e^{\\beta_2}$ if you are the (potentially) discriminated class.\n",
    "* $e^0 = 1$ if you are not the (potentially) discriminated class.\n",
    "\n",
    "Thus, given a dataset of salaries at a company, we might seek to estimate $e^{\\beta_2\\text{class}}$ to prove/disprove pay equity discrimination. Using the power of Bayesian statistics, we can actually begin with a prior assumption of _no_ pay inequity in order to allow the data to contradict it if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wages = pd.read_csv(\"../datasets/wages.csv\")\n",
    "wages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: What are the priors and likelihood?\n",
    "We actually have _four_ variables to set priors on here. Most of them we actually don't have any relevant prior information for. We might set these as **noninformative priors** - that is, priors that are incredibly vague and provide no information:\n",
    "\n",
    "* $\\beta_0$ and $\\beta_1$ - noninformative, we have no prior information.\n",
    "* $\\sigma$ - the model's variance - also noninformative, we have no good guesses.\n",
    "* $\\beta_2$ - the coefficient of protected class. We'll put a prior that assumes $\\beta_2 \\approx 0$ in order to assume innocence.\n",
    "\n",
    "For the likelihood, remember our parametrization of OLS as a GLM:\n",
    "\n",
    "$$Y = \\mathbf{x}^T\\beta + \\varepsilon$$\n",
    "\n",
    "where $\\varepsilon \\sim N(0, \\sigma)$, and so\n",
    "\n",
    "$$ Y \\sim N(\\mathbf{x}^T\\beta, \\sigma) $$\n",
    "\n",
    "So we can write \n",
    "\n",
    "$$ \\log W \\sim N(\\beta_0 + \\beta_1\\text{skill} + \\beta_2\\text{class}, \\sigma) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta_0 ~ Normal(0, 100)\n",
    "# beta_1 ~ Normal(0, 100)\n",
    "# beta_2 ~ Normal(0, 0.1)\n",
    "# sigma ~ InverseGamma(0.001, 0.001) --> only positive numbers, convention to use inv. gamma (computational advtgs)\n",
    "# Y ~ N(beta_0 + beta_1*skill + beta_2*class, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: PyMC3 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Priors:\n",
    "    beta_0 = pm.Normal(\"beta_0\", 0, 100)\n",
    "    beta_1 = pm.Normal(\"beta_1\", 0, 100)\n",
    "    beta_2 = pm.Normal(\"beta_2\", 0, 0.1)\n",
    "    sigma = pm.InverseGamma(\"sigma\", 0.01, 0.01)\n",
    "    # Likelihood:\n",
    "    Y = pm.Normal(\n",
    "        \"log_wage\",\n",
    "        mu=beta_0 + beta_1*wages[\"skill\"] + beta_2*wages[\"class\"],\n",
    "        sd=sigma,\n",
    "        observed=np.log(wages[\"wage\"])\n",
    "    )\n",
    "    trace = pm.sample(10000, tune=1000, init=\"advi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    pm.traceplot(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = np.exp(trace[\"beta_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*(np.mean(coef)-1)\n",
    "# Interpretation:\n",
    "# if we are in class 1, we’d expect our wages to drop by 15.85 percent holding skill constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View distribution of expected decrease in wages\n",
    "plt.hist(100*(coef - 1), bins = 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*(max(coef)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to `statsmodels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X = wages.loc[:, [\"skill\", \"class\"]]\n",
    "X = sm.add_constant(X)\n",
    "y = np.log(wages[\"wage\"])\n",
    "lm = sm.OLS(y,X).fit()\n",
    "lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was actually simulated to be 0.8\n",
    "np.exp(-0.1853)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*(np.exp(-0.1853) - 1)\n",
    "# Interpretation:\n",
    "# if we are in class 1, we’d expect our wages to drop by 16.91 percent holding skill constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
